{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4484ec-9a7d-423e-b6fa-eaf9409a6330",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dotenv\n",
    "from envs import env\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666bb8e3-be38-4477-9d1d-c0fd8fb184e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c80a540-9bda-438a-b90d-aec021c6dd51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import SeleniumURLLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3ea473-dfde-454f-8029-846f8fe475b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "loader = DirectoryLoader('/workspace/carcamp', glob=\"**/*.html\", silent_errors=True, load_hidden=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62086143-2174-4d39-b8c6-a9d32d25909f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011b3b4e-4201-4158-8461-b3f983cb882b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pinecone \n",
    "\n",
    "# initialize pinecone\n",
    "pinecone.init(\n",
    "    api_key=env(\"PINECONE_API_KEY\"),  # find at app.pinecone.io\n",
    "    environment=env(\"PINECONE_ENV\")  # next to api key in console\n",
    ")\n",
    "\n",
    "index_name = \"codeagi\"\n",
    "\n",
    "docsearch = Pinecone.from_documents(docs, embeddings, index_name=index_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eacb54-9bce-44a6-a1f0-3845d6413954",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"class Dealer\"\n",
    "res_docs = docsearch.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c21de1-c02f-4bd6-9a9d-89f316df1c98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain.run(input_documents=res_docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16950e92-8c58-43c4-bf33-ccf1db15d54c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Use the context below to use write Python code about the following topic:\n",
    "    Context: {context}\n",
    "    Task: {topic}\n",
    "    Code:\n",
    "    ```{language}\n",
    "    \"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"topic\", \"language\"]\n",
    ")\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=PROMPT, max_tokens_limit=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17151100-c83f-4b7e-aa62-0fc147fe7ece",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_code(topic, language):\n",
    "    docs = docsearch.similarity_search(topic, k=4)\n",
    "    inputs = [{\"context\": doc.page_content, \"topic\": topic, \"language\": language} for doc in docs]\n",
    "    txt = chain.apply(inputs)[0]['text']\n",
    "    return txt.replace(\"'''\", \"\")\n",
    "    # return f\"\"\"```{language} {txt}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "81e8c719-915e-4b38-87ca-e557dc7e6c50",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nclass Dealer(NameSlugModel):\\n    street_address = models.CharField(max_length=200)\\n    city = models.CharField(max_length=100)\\n    state = models.CharField(max_length=100)\\n    zip_code = models.PositiveIntegerField()\\n    phone_number = models.CharField(max_length=30, blank=True, null=True)\\n    email = models.EmailField()\\n    dealer_number = models.CharField(max_length=100)\\n    stripe_customer_id = models.UUIDField(blank=True, null=True)\\n    aws_api_key = models.UUIDField(blank=True, null=True)\\n    status = models.CharField(choices=DEALER_STATUS, default='unpaid', max_length=200)\\n    website = models.URLField(blank=True, null=True)\\n    ```\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_code('Rewrite the Dealer Django model to be more efficient', 'python')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c250b4c-5819-44d7-b207-db1da318977a",
   "metadata": {},
   "source": [
    "## Delete the CodeAGI Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8f2d74-cc10-4edd-9625-5b3e6a36c896",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pinecone.Index(index_name=index_name).delete(delete_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b670d3a-c293-4f16-b0c2-37ebd31e796a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f3b0ff-7e35-45b5-a66f-55f3a12938ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "enc.encode('Test statement')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4cac6b-0eff-48f0-8e84-7305efff27b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "st = \"\"\"\n",
    "## Original Author\n",
    "\n",
    "The writer's main points are that vector databases are popular for large language models, but there is a cost to introducing new infrastructure; however, the writer suggests that it might not be necessary to have a vector database. He argues that for N entities, it is possible to calculate the top k most similar entities to a given entity with O(N) complexity instead of pre-calculating and storing in a vector database, which takes O(N^2) complexity. He performed a benchmark to demonstrate the effect of the O(N) complexity and concluded that the O(N) complexity could be reasonable depending on the size of N and the latency requirements. He then discussed potential counterarguments before concluding that the right tool for the job is usually the tool you already have.\n",
    "\n",
    "## Replies\n",
    "\n",
    "\t\n",
    "jeadie 4 days ago | next [–]\n",
    "\n",
    "I'm skeptical about some vector databases these days, but your article misses a few import points when it comes to LLMs.\n",
    "1. To use LLMs effectively, you often need to generate and store more than 1 vector per document. 10 million vectors may only be 100,000 documents. This may still be enough for alot of small problems. 2. Pgvector currently has great limitations on recall/latency because underlying its ANN its using IVF (I'm currently working on adding HNSW-IVF and HNSW support to PGVector). In some cases, even elasticsearch can have issues with scale (the problem comes from the constraint of one ANN index per index segment, and immutability). 3. Pre-calculate seems like the wrong word to describe HNSW graph construction.\n",
    "\n",
    "I think a point you miss that is important to consider for LLM + vector DBs is the fact that so much of the complexity of these uses cases cannot be captured by the vector DB (e.g. pinecone, chroma, qdrant, etc). I think there are some more end to end systems, at least in search, attempting to solve this (e.g Marqo, maybe Weaviate). Overall, I like the article. It makes a worthwhile claim and counterpoint to all the vector DB hype.\n",
    "\n",
    "reply\n",
    "\n",
    "\t\n",
    "kacperlukawski 4 days ago | parent | next [–]\n",
    "\n",
    "I'd love to hear more about your thoughts on the complexity that cannot be, in your opinion, captured by the vector DB. I probably didn't get your point.\n",
    "Disclaimer: I work for Qdrant, and we believe a database should be just a database. I remember attempting to move logic to the database layer and coupling neural encoders into the vector database sounds the same.\n",
    "\n",
    "reply\n",
    "\n",
    "\t\n",
    "isoprophlex 4 days ago | parent | prev | next [–]\n",
    "\n",
    "> 1. To use LLMs effectively, you often need to generate and store more than 1 vector per document.\n",
    "Could you elaborate this point for me? What would cause the 1 document -> ~ 100 vectors blowup; do you store vector embeddings for sections of the document, or use multiple models to create several types of vectors?\n",
    "\n",
    "reply\n",
    "\n",
    "\t\n",
    "Dachande663 4 days ago | root | parent | next [–]\n",
    "\n",
    "If you look at something like LangChain[0], it supports/recommends splitting larger documents into smaller chunks. In this way, when doing something like semantic search you can get the specific paragraph/section that holds the closest relevance, rather than having to read the entire document again (think of a 100 page PDF).\n",
    "https://python.langchain.com/en/latest/modules/indexes/text_...\n",
    "\n",
    "reply\n",
    "\n",
    "\t\n",
    "jeadie 4 days ago | root | parent | prev | next [–]\n",
    "\n",
    "This is generally very context/use case specific. In general, if a document is a `Dict[str, Any]`, then you either have to have one (or multiple) vector(s) per field, unless you want to combine vectors across fields (it's not self-evident how you'd best do that). In saying that, specific reason's to do this (or why I've done it in the past).\n",
    "1. Chunking long text fields in documents so as to get a better semantic vector for them (also you can only fit so much into an LLM). 2. Differently to 1. chunking long text fields (or even chunking images, audio, etc), is one way to perform highlighting. It helps to answer the question, for example, for a given document what about it was the reason it was returned? You can then point to the area in the image/text/audio that was most relevant. 3. You may want to run different LLMs on different fields (perhaps a separate multi-modal LLM vs a standard text LLM), or like another comment said have different transforms/representations of the same field.\n",
    "\n",
    "Perhaps 100 vectors is non-standard, but definitely not unseen.\n",
    "\n",
    "reply\n",
    "\n",
    "\t\n",
    "jkb79 4 days ago | root | parent | next [–]\n",
    "\n",
    "Only Vespa allows you to index multiple vectors per schema field, avoiding duplicating all the meta data of the document into the \"chunk\", and avoids maintaining the document to chunk fan-out. See https://blog.vespa.ai/semantic-search-with-multi-vector-inde...\n",
    "reply\n",
    "\n",
    "\t\n",
    "Sai_ 4 days ago | root | parent | prev | next [–]\n",
    "\n",
    "I’m not a data scientist but I think I know why one document could lead to many vectors.\n",
    "(Happy to be corrected and/or schooled.)\n",
    "\n",
    "A vector is a list of numbers each of which represents weight accorded to a certain word along a certain dimension.\n",
    "\n",
    "Let’s take an example.\n",
    "\n",
    "Is an “apple” a “positive” or a “negative” thing? Most people would associate positivity with apples. So, for the general population, the vector for “apple” along the 0-1 continuum where 0 represents negative sentiment and 1 represents positive sentiment would be something like [0.8].\n",
    "\n",
    "Let’s add one more dimension. Is an apple associated with computers (1) or not (0)? For the majority of the world where Windows has a massive market share, “apple” would recall a fruit, not a sleek laptop. Therefore, the vector for apple along the computer/non-computer dimension is probably [0.3].\n",
    "\n",
    "Taking this together, apple = [0.8, 0.3] where positionally, 0.8 is the value for positive/negative sentiment while 0.3 is computer/non-computer.\n",
    "\n",
    "Agree?\n",
    "\n",
    "(Hoping you do)\n",
    "\n",
    "But that [0.8, 0.3] vector is for the general population.\n",
    "\n",
    "Would a bible literalist who publishes blogs on bible stories feel the same way?\n",
    "\n",
    "For someone like that, the notion of the original sin could taint their sentiments towards the apple. So they might weight an apple at 0.2 on the positive/negative line. Since they’re bloggers, it’s more likely they associate apple with computers so they might call it 0.5. Therefore, their apple vector is [0.2, 0.5].\n",
    "\n",
    "Extend this to more content and you’ll see why there are more than one vector.\n",
    "\n",
    "At least that’s how I understood it. Happy to be corrected and/or schooled.\n",
    "\n",
    "reply\n",
    "\n",
    "\t\n",
    "ruslandanilin 4 days ago | root | parent | next [–]\n",
    "\n",
    "In my opinion, you could represent \"apple\" as a vector, for example, [0.99, 0.3, 0.7] in relation to [fruits, computers, religion]. Then, you can create different user vectors that describe the interests of various groups. For instance, the general population might have a vector like [0.8, 0.2, 0.1], geeks as [0.6, 0.95, 0.05], and religious people as [0.7, 0.1, 0.95].\n",
    "By creating these user vectors, you can compare them with the \"apple\" vector and find the best match using ANN. This approach allows you to determine which group is most interested in a given context or aspect of the word \"apple.\" The ANN will help you identify similarities or patterns in the user vectors and the \"apple\" vector to find the most relevant matches.\n",
    "\n",
    "Thank you\n",
    "\n",
    "reply\n",
    "\n",
    "\t\n",
    "Sai_ 3 days ago | root | parent | next [–]\n",
    "\n",
    "I don’t know what ANN is but your comment raises two questions in my mind -\n",
    "1. Where did your first vector of [0.99, 0.3, 0.7] come from? You later present the concept of user vectors which are vectors for different cohorts of users but don’t name the first vector as a user vector.\n",
    "\n",
    "2. I feel my example of vectors for “general population users” and “bible literalist blogger” user aligns with your “user vector” concept.\n",
    "\n",
    "reply\n",
    "\n",
    "\t\n",
    "thanatropism 4 days ago | root | parent | prev | next [–]\n",
    "\n",
    "Modern text embeddings are not word-based like that.\n",
    "reply\n",
    "\n",
    "\t\n",
    "Sai_ 3 days ago | root | parent | next [–]\n",
    "\n",
    "If my understanding and explanation are directionally correct, I’m happy. I’ll be the first one to admit I’m not a data scientist.\n",
    "Do you have a good example of how an actual data scientist would present the idea of vectors as applied to sentences/documents to a layperson?\n",
    "\n",
    "reply\n",
    "\n",
    "\t\n",
    "teaearlgraycold 4 days ago | root | parent | prev | next [–]\n",
    "\n",
    "Storing one document as one embedding is like making a movie poster the average of all frames in the film.\n",
    "reply\n",
    "\n",
    "\t\n",
    "jkb79 4 days ago | root | parent | next [–]\n",
    "\n",
    "That is a very good analogy!\n",
    "reply\n",
    "\n",
    "\t\n",
    "teaearlgraycold 3 days ago | root | parent | next [–]\n",
    "\n",
    ":D Thanks!\n",
    "reply\n",
    "\n",
    "\t\n",
    "amitport 4 days ago | root | parent | prev | next [–]\n",
    "\n",
    "One thing others didn't mention is that \"document\" is a general term but in some cases (e.g., question answering) the typical document can be a very short paragraph and take much less memory than the vector. Also note that with some ML architecture the vector is very large (e.g., an entire very layer output)\n",
    "reply\n",
    "\n",
    "\t\n",
    "ethanahte 4 days ago | parent | prev | next [–]\n",
    "\n",
    "Hi, author here.\n",
    "1. You make a great point about longer documents requiring multiple vectors which I should've mentioned in the post. Depending on your use case, this can certainly explode your dataset size! 2. Good to know about the pgvector limitations -- I haven't used it yet. 3. I guess \"index\" would be the more database-y term. That said, one thing I'll call out is that you have to re-index if you ever change your embedding model, and indexing can be slow. It took me ~20-30 minutes to index the 10 million embeddings in my benchmark.\n",
    "\n",
    "reply\n",
    "\n",
    "\t\n",
    "kordlessagain 4 days ago | root | parent | next [–]\n",
    "\n",
    "I'm interested if anyone has some hard data on the \"best\" size of the document \"fragments\" that are used for embedding into a dense vector.\n",
    "Obviously, embedding single words probably aren't particularly useful for reassembling portions of a document for submission to an LLM in the prompt. I'm currently pondering on what size of string is best for embedding, and considering a variable size might be one option.\n",
    "\n",
    "Testing with strings around 512 characters seem to do pretty well, but it may be storing multiple lengths of similar runs in the document might be a better way to do it.\n",
    "\n",
    "reply\n",
    "\n",
    "Summarize the text above.\n",
    "\n",
    "The original author discussed the cost of introducing new infrastructure for vector databases, suggesting that it might not be necessary if one can calculate the most similar entities with O(N) instead of O(N^2) complexity. Other commenters discussed the complexity of LLMs and vector databases, the need to generate more than one vector per document, and the benefits of chunking documents for semantic search. They also compared different user vectors and discussed the need for ML architectures with large vectors. In conclusion, the right tool for the job is usually the tool you already have.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb0897b-fc34-4031-886a-cfa4f88d33a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "enc.encode(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7999c9-7890-4f50-a640-7e1d112bf2f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def chunk_text(text, limit): \n",
    "    chunked_text = [] \n",
    "    current_chunk = \"\" \n",
    "    for word in text.split(): \n",
    "        current_chunk += word + \" \" \n",
    "        if len(current_chunk) > limit: \n",
    "            chunked_text.append(current_chunk) \n",
    "            current_chunk = \"\" \n",
    "    if current_chunk != \"\": \n",
    "        chunked_text.append(current_chunk) \n",
    "    return chunked_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62e6e43-d21b-4df6-971c-fed86bea77be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunk_text(st, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16f97a0-0c64-41ca-a8c7-a5fccac944f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
